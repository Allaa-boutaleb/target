{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 05:26:10.778185: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-17 05:26:10.781789: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-17 05:26:10.834058: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-17 05:26:11.941023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Fact Verification Task',\n",
       " 'Table Question Answering Task',\n",
       " 'Table Retrieval Task',\n",
       " 'Text to SQL Task']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from target_benchmark.evaluators import TARGET, get_task_names\n",
    "get_task_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fetaqa': HFDatasetConfigDataModel(dataset_name='fetaqa', split='test', data_directory=None, query_type='Table Question Answering', aux=None, hf_corpus_dataset_path='target-benchmark/fetaqa-corpus', hf_queries_dataset_path='target-benchmark/fetaqa-queries'),\n",
       " 'ottqa': HFDatasetConfigDataModel(dataset_name='ottqa', split='validation', data_directory=None, query_type='Table Question Answering', aux=None, hf_corpus_dataset_path='target-benchmark/ottqa-corpus', hf_queries_dataset_path='target-benchmark/ottqa-queries'),\n",
       " 'tabfact': HFDatasetConfigDataModel(dataset_name='tabfact', split='test', data_directory=None, query_type='Fact Verification', aux=None, hf_corpus_dataset_path='target-benchmark/tabfact-corpus', hf_queries_dataset_path='target-benchmark/tabfact-queries'),\n",
       " 'infiagentda': HFDatasetConfigDataModel(dataset_name='infiagentda', split='test', data_directory=None, query_type='Other', aux=None, hf_corpus_dataset_path='target-benchmark/infiagentda-corpus', hf_queries_dataset_path='target-benchmark/infiagentda-queries'),\n",
       " 'spider-test': Text2SQLDatasetConfigDataModel(dataset_name='spider-test', split='test', data_directory=None, query_type='Text to SQL', aux=None, hf_corpus_dataset_path='target-benchmark/spider-corpus-test', hf_queries_dataset_path='target-benchmark/spider-queries-test'),\n",
       " 'bird-validation': Text2SQLDatasetConfigDataModel(dataset_name='bird-validation', split='test', data_directory=None, query_type='Text to SQL', aux=None, hf_corpus_dataset_path='target-benchmark/bird-corpus-validation', hf_queries_dataset_path='target-benchmark/bird-queries-validation')}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from target_benchmark.tasks import TableRetrievalTask\n",
    "TableRetrievalTask._get_default_dataset_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fetaqa = TARGET((\"Table Retrieval Task\", \"fetaqa\"))\n",
    "target_ottqa = TARGET((\"Table Retrieval Task\", \"ottqa\"))\n",
    "target_tabfact = TARGET((\"Table Retrieval Task\", \"tabfact\"))\n",
    "target_spider = TARGET((\"Table Retrieval Task\", \"spider-test\"))\n",
    "target_bird = TARGET((\"Table Retrieval Task\", \"bird-validation\"))\n",
    "target_infiagentda = TARGET((\"Table Retrieval Task\", \"infiagentda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import LlamaIndexRetriever\n",
    "llamaindex_retriever = LlamaIndexRetriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = target_fetaqa.run(llamaindex_retriever, \"validation\", top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=5, accuracy=0.44555444555444557, precision=None, recall=None, retrieval_time=106.81327, avg_retrieval_time=0.10671), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'bleu': {'bleu': 0.12774350688956024, 'precisions': [0.35200152134639157, 0.17261518494484102, 0.10533781653882526, 0.0676979374584165], 'brevity_penalty': 0.8854071251608363, 'length_ratio': 0.8914978384335001, 'translation_length': 21034, 'reference_length': 23594}, 'sacrebleu': {'score': 12.774350688956028, 'counts': [7404, 3458, 2005, 1221], 'totals': [21034, 20033, 19034, 18036], 'precisions': [35.200152134639154, 17.2615184944841, 10.533781653882526, 6.76979374584165], 'bp': 0.8854071251608363, 'sys_len': 21034, 'ref_len': 23594}, 'rouge': {'rouge1': 0.2797871142743029, 'rouge2': 0.16322956616548168, 'rougeL': 0.22260816489324553, 'rougeLsum': 0.22779389901592179}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=43.32646, avg_embedding_creation_duration=0.04328, embedding_size=3.52666, avg_embedding_size=0.00352))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.5454545454545454, precision=None, recall=None, retrieval_time=105.64053, avg_retrieval_time=0.10553), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'rouge': {'rouge1': 0.3416405652971404, 'rouge2': 0.20253177988132284, 'rougeL': 0.2754744438590071, 'rougeLsum': 0.2818891819491625}, 'sacrebleu': {'score': 14.875547927665979, 'counts': [8880, 4277, 2498, 1525], 'totals': [24841, 23840, 22841, 21844], 'precisions': [35.74735316613663, 17.940436241610737, 10.936473884681057, 6.981322102179088], 'bp': 1.0, 'sys_len': 24841, 'ref_len': 23594}, 'bleu': {'bleu': 0.1487554792766598, 'precisions': [0.3574735316613663, 0.17940436241610738, 0.10936473884681056, 0.06981322102179088], 'brevity_penalty': 1.0, 'length_ratio': 1.0528524201068068, 'translation_length': 24841, 'reference_length': 23594}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=42.41042, avg_embedding_creation_duration=0.04237, embedding_size=-0.00819, avg_embedding_size=-1e-05))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetaqa Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = target_fetaqa.run(llamaindex_retriever, \"test\", top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.44782825761357964, precision=None, recall=None, retrieval_time=414.04305, avg_retrieval_time=0.20671), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'sacrebleu': {'score': 12.834750303393088, 'counts': [16461, 7853, 4664, 2900], 'totals': [53432, 51429, 49431, 47434], 'precisions': [30.807381344512653, 15.269594975597425, 9.435374562521494, 6.113758063836067], 'bp': 1.0, 'sys_len': 53432, 'ref_len': 46919}, 'rouge': {'rouge1': 0.30684692336094793, 'rouge2': 0.18092410771505416, 'rougeL': 0.2514185751479096, 'rougeLsum': 0.2551126236872021}, 'bleu': {'bleu': 0.1283475030339309, 'precisions': [0.3080738134451265, 0.15269594975597425, 0.09435374562521495, 0.06113758063836067], 'brevity_penalty': 1.0, 'length_ratio': 1.1388137002067393, 'translation_length': 53432, 'reference_length': 46919}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=163.07156, avg_embedding_creation_duration=0.08141, embedding_size=-6.47987, avg_embedding_size=-0.00324))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTTQA Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_llama_ottqa_val = target_ottqa.run(llamaindex_retriever, \"validation\", top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabfact Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:target_benchmark.evaluators.TARGET:Started creating data loader objects...\n",
      "INFO:target_benchmark.evaluators.TARGET:Start running Fact Verification Task...\n",
      "INFO:target_benchmark.evaluators.TARGET:Start checking for new corpus to embed...\n"
     ]
    }
   ],
   "source": [
    "results_llama_tabfact_test = target_tabfact.run(llamaindex_retriever, \"test\", top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_llama_tabfact_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Naive OAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import OpenAIEmbedder\n",
    "oai_embedder = OpenAIEmbedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetaqa Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Tables...: 100%|██████████| 2003/2003 [04:34<00:00,  7.30it/s]\n",
      "Retrieving Tables for fetaqa...: 100%|██████████| 2003/2003 [04:47<00:00,  6.96it/s]\n"
     ]
    }
   ],
   "source": [
    "results_oai_fetaqa_test = target_fetaqa.run(oai_embedder, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./oai_fetaqa_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.7234148776834748, precision=None, recall=None, retrieval_time=1009.69437, avg_retrieval_time=0.50409), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=15.81088, avg_embedding_creation_duration=0.00789, embedding_size=24612864.0, avg_embedding_size=12288.0))}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_oai_fetaqa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTTQA Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Tables...: 100%|██████████| 789/789 [01:58<00:00,  6.64it/s]\n",
      "Retrieving Tables for ottqa...: 100%|██████████| 2214/2214 [05:09<00:00,  7.16it/s]\n"
     ]
    }
   ],
   "source": [
    "results_oai_ottqa_val = target_ottqa.run(oai_embedder, \"validation\", top_k = 10, batch_size=100, retrieval_results_file=\"./oai_ottqa_val_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'ottqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.9498644986449865, precision=None, recall=None, retrieval_time=1090.10155, avg_retrieval_time=0.49237), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=6.12045, avg_embedding_creation_duration=0.00776, embedding_size=9695232.0, avg_embedding_size=12288.0))}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_oai_ottqa_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabfact Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Tables...:   0%|          | 0/1695 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Tables...: 100%|██████████| 1695/1695 [04:00<00:00,  7.04it/s]\n",
      "Retrieving Tables for tabfact...: 100%|██████████| 12779/12779 [29:24<00:00,  7.24it/s]\n"
     ]
    }
   ],
   "source": [
    "results_oai_tabfact_test = target_tabfact.run(oai_embedder, \"test\", top_k=10, batch_size=100, retrieval_results_file=\"oai_tabfact_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'tabfact': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.7829251115110728, precision=None, recall=None, retrieval_time=6376.56668, avg_retrieval_time=0.49899), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=13.96042, avg_embedding_creation_duration=0.00824, embedding_size=20828160.0, avg_embedding_size=12288.0))}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_oai_tabfact_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fc72b47ffd474991a1f14d2e3765df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 132 files:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Tables...: 100%|██████████| 180/180 [00:24<00:00,  7.27it/s]\n",
      "Retrieving Tables for spider-test...: 100%|██████████| 2147/2147 [04:30<00:00,  7.95it/s]\n"
     ]
    }
   ],
   "source": [
    "results_oai_spider_test = target_spider.run(oai_embedder, \"test\", top_k=10, batch_size=100, retrieval_results_file=\"./oai_spider_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'spider-test': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.0, precision=None, recall=None, retrieval_time=1028.19846, avg_retrieval_time=0.4789), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=1.64523, avg_embedding_creation_duration=0.00914, embedding_size=2211840.0, avg_embedding_size=12288.0))}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_oai_spider_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infiagentda Test\n",
    "DON'T INCLUDE FOR NOW?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_oai_infiagentda_test = target_infiagentda.run(oai_embedder, \"test\", top_k=10, batch_size=100, retrieval_results_file=\"./oai_infiagentda_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HNSW OAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import HNSWOpenAIEmbeddingRetriever\n",
    "hnsw_oai = HNSWOpenAIEmbeddingRetriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetaqa Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:39, 11.99s/it]"
     ]
    }
   ],
   "source": [
    "results_hnsw_oai_fetaqa_test = target_fetaqa.run(hnsw_oai, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./hnsw_oai_fetaqa_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.12431352970544184, precision=None, recall=None, retrieval_time=24.25388, avg_retrieval_time=0.01211), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'rouge': {'rouge1': 0.0943664578019575, 'rouge2': 0.049193712730089476, 'rougeL': 0.07689053600356258, 'rougeLsum': 0.07803228871432272}, 'sacrebleu': {'score': 3.4375341088803752, 'counts': [6235, 1989, 1080, 620], 'totals': [27543, 25540, 23539, 21538], 'precisions': [22.637330719239007, 7.78778386844166, 4.588130336887718, 2.8786331135667194], 'bp': 0.49485928556419767, 'sys_len': 27543, 'ref_len': 46919}, 'bleu': {'bleu': 0.034375341088803746, 'precisions': [0.22637330719239007, 0.0778778386844166, 0.045881303368877184, 0.028786331135667195], 'brevity_penalty': 0.49485928556419767, 'length_ratio': 0.5870329717172148, 'translation_length': 27543, 'reference_length': 46919}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=8.7827, avg_embedding_creation_duration=0.00438, embedding_size=12.72218, avg_embedding_size=0.00635))}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_hnsw_oai_fetaqa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTTQA Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving Tables for ottqa...: 100%|██████████| 2214/2214 [04:54<00:00,  7.51it/s]\n"
     ]
    }
   ],
   "source": [
    "results_hnsw_oai_ottqa_test = target_ottqa.run(hnsw_oai, \"validation\", top_k = 10, batch_size=100, retrieval_results_file=\"./hnsw_oai_ottqa_val_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'ottqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.950316169828365, precision=None, recall=None, retrieval_time=21.10932, avg_retrieval_time=0.00953), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=3e-05, avg_embedding_creation_duration=0.0, embedding_size=0.0, avg_embedding_size=0.0))}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_hnsw_oai_ottqa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabfact Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [03:41, 13.05s/it]\n",
      "Retrieving Tables for tabfact...: 100%|██████████| 12779/12779 [27:33<00:00,  7.73it/s]\n"
     ]
    }
   ],
   "source": [
    "results_hnsw_oai_tabfact_test = target_tabfact.run(hnsw_oai, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./hnsw_oai_tabfact_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'tabfact': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.7807340167462242, precision=None, recall=None, retrieval_time=105.79046, avg_retrieval_time=0.00828), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=5.26515, avg_embedding_creation_duration=0.00311, embedding_size=10.73562, avg_embedding_size=0.00633))}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_hnsw_oai_tabfact_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7d5a2aaedb4ba190979d0d1e446a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 132 files:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:16, 16.86s/it]\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 39370 tokens (39370 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_hnsw_oai_spider_test \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_spider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhnsw_oai\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieval_results_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./hnsw_oai_spider_test_retrieval_results.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/carl/target/target_benchmark/evaluators/TARGET.py:485\u001b[0m, in \u001b[0;36mTARGET.run\u001b[0;34m(self, retriever, split, batch_size, top_k, retrieval_results_file, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m     (\n\u001b[1;32m    479\u001b[0m         duration,\n\u001b[1;32m    480\u001b[0m         embedding_size,\n\u001b[1;32m    481\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_with_standardized_embeddings(\n\u001b[1;32m    482\u001b[0m         retriever, dataset_name, client\n\u001b[1;32m    483\u001b[0m     )\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m     duration, embedding_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_with_custom_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m loaded_datasets\u001b[38;5;241m.\u001b[39madd(dataset_name)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# create embedding statistics data object to record latency & size of embedding\u001b[39;00m\n",
      "File \u001b[0;32m~/carl/target/target_benchmark/evaluators/TARGET.py:407\u001b[0m, in \u001b[0;36mTARGET.embed_with_custom_embeddings\u001b[0;34m(self, retriever, dataset_name, batch_size)\u001b[0m\n\u001b[1;32m    404\u001b[0m start_disk_usage \u001b[38;5;241m=\u001b[39m shutil\u001b[38;5;241m.\u001b[39mdisk_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mused\n\u001b[1;32m    405\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m--> 407\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_corpus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_corpus_table_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_expected_corpus_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m    414\u001b[0m duration \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/carl/target/target_benchmark/retrievers/naive/HNSWOpenAIEmbeddingRetriever.py:123\u001b[0m, in \u001b[0;36mHNSWOpenAIEmbeddingRetriever.embed_corpus\u001b[0;34m(self, dataset_name, corpus)\u001b[0m\n\u001b[1;32m    121\u001b[0m         tup_id \u001b[38;5;241m=\u001b[39m (db_id, table_id)\n\u001b[1;32m    122\u001b[0m         table_str \u001b[38;5;241m=\u001b[39m markdown_table_str(table, num_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rows)\n\u001b[0;32m--> 123\u001b[0m         embedded_corpus[tup_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m corpus_index \u001b[38;5;241m=\u001b[39m construct_embedding_index(\u001b[38;5;28mlist\u001b[39m(embedded_corpus\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Store table embedding index and table ids in distinct files\u001b[39;00m\n",
      "File \u001b[0;32m~/carl/target/target_benchmark/retrievers/naive/HNSWOpenAIEmbeddingRetriever.py:86\u001b[0m, in \u001b[0;36mHNSWOpenAIEmbeddingRetriever.embed_query\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 86\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding\n",
      "File \u001b[0;32m~/miniconda3/envs/target/lib/python3.12/site-packages/openai/resources/embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/target/lib/python3.12/site-packages/openai/_base_client.py:1260\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1248\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1255\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1257\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1258\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1259\u001b[0m     )\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/target/lib/python3.12/site-packages/openai/_base_client.py:937\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    930\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/target/lib/python3.12/site-packages/openai/_base_client.py:1041\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1040\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1044\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1045\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1050\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 39370 tokens (39370 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "results_hnsw_oai_spider_test = target_spider.run(hnsw_oai, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./hnsw_oai_spider_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_hnsw_oai_spider_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTTQA TFIDF with title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import OTTQARetriever\n",
    "tfidf_with_title = OTTQARetriever(encoding=\"tfidf\", withtitle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetaqa Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 31.79it/s]00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 28.16it/s]\n",
      "Retrieving Tables for fetaqa...: 100%|██████████| 2003/2003 [00:01<00:00, 1113.33it/s]\n"
     ]
    }
   ],
   "source": [
    "results_tfidf_with_title_fetaqa_test = target_fetaqa.run(tfidf_with_title, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./tfidf_title_fetaqa_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.08287568647029456, precision=None, recall=None, retrieval_time=1.47672, avg_retrieval_time=0.00074), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=4.32525, avg_embedding_creation_duration=0.00216, embedding_size=1.1223, avg_embedding_size=0.00056))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_tfidf_with_title_fetaqa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTTQA Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 145.23it/s]0:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 94.72it/s]\n",
      "Retrieving Tables for ottqa...: 100%|██████████| 2214/2214 [00:02<00:00, 887.92it/s]\n"
     ]
    }
   ],
   "source": [
    "results_tfidf_title_ottqa_val = target_ottqa.run(tfidf_with_title, \"validation\", top_k = 10, batch_size=100, retrieval_results_file=\"./tfidf_title_ottqa_val_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'ottqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.950316169828365, precision=None, recall=None, retrieval_time=2.03999, avg_retrieval_time=0.00092), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=3.27222, avg_embedding_creation_duration=0.00415, embedding_size=-0.70451, avg_embedding_size=-0.00089))}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_tfidf_title_ottqa_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabfact Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 61.14it/s]00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 49.99it/s]\n",
      "Retrieving Tables for tabfact...: 100%|██████████| 12779/12779 [00:12<00:00, 1022.06it/s]\n"
     ]
    }
   ],
   "source": [
    "results_tfidf_with_title_tabfact_test = target_tabfact.run(tfidf_with_title, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./tfidf_with_title_tabfact_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'tabfact': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.3359417794819626, precision=None, recall=None, retrieval_time=9.86623, avg_retrieval_time=0.00077), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=4.04473, avg_embedding_creation_duration=0.00239, embedding_size=0.49152, avg_embedding_size=0.00029))}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_tfidf_with_title_tabfact_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a60da051ee44d3b4504eb98a4d5221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 132 files:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 472.12it/s]0:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 187.89it/s]\n",
      "Retrieving Tables for spider-test...: 100%|██████████| 2147/2147 [00:25<00:00, 84.36it/s] \n"
     ]
    }
   ],
   "source": [
    "results_tfidf_with_title_spider_test = target_spider.run(tfidf_with_title, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./tfidf_with_title_spider_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'spider-test': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.856078248719143, precision=None, recall=None, retrieval_time=1.99155, avg_retrieval_time=0.00093), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=3.53082, avg_embedding_creation_duration=0.01962, embedding_size=-0.86016, avg_embedding_size=-0.00478))}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_tfidf_with_title_spider_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIRD Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab56dc985e2a4e208d97b9ac800b51ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 104 files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 1346.49it/s]:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 193.72it/s]\n",
      "Retrieving Tables for bird-validation...:  39%|███▉      | 600/1534 [52:56<1:34:48,  6.09s/it]"
     ]
    }
   ],
   "source": [
    "results_tfidf_with_title_bird_val = target_bird.run(tfidf_with_title, \"validation\", top_k = 10, batch_size=100, retrieval_results_file=\"./tfidf_with_title_bird_val_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_tfidf_with_title_bird_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults_tfidf_with_title_bird_val\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_tfidf_with_title_bird_val' is not defined"
     ]
    }
   ],
   "source": [
    "results_tfidf_with_title_bird_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTTQA bm25 with Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import OTTQARetriever\n",
    "bm25_with_title = OTTQARetriever(encoding=\"bm25\", withtitle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetaqa Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 61.45it/s]00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 50.73it/s]\n",
      "Retrieving Tables for fetaqa...: 100%|██████████| 2003/2003 [00:01<00:00, 1038.15it/s]\n"
     ]
    }
   ],
   "source": [
    "results_bm25_with_title_fetaqa_test = target_fetaqa.run(bm25_with_title, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./bm25_with_title_fetaqa_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.08237643534697953, precision=None, recall=None, retrieval_time=1.51352, avg_retrieval_time=0.00076), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=4.59571, avg_embedding_creation_duration=0.00229, embedding_size=-0.01229, avg_embedding_size=-1e-05))}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bm25_with_title_fetaqa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTTQA Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 146.48it/s]0:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 97.50it/s]\n",
      "Retrieving Tables for ottqa...: 100%|██████████| 2214/2214 [00:02<00:00, 841.97it/s]\n"
     ]
    }
   ],
   "source": [
    "results_bm25_with_title_ottqa_val = target_ottqa.run(bm25_with_title, \"validation\", top_k = 10, batch_size=100, retrieval_results_file=\"./bm25_with_title_ottqa_val_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'ottqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.9552845528455285, precision=None, recall=None, retrieval_time=2.05174, avg_retrieval_time=0.00093), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=3.52938, avg_embedding_creation_duration=0.00447, embedding_size=0.03277, avg_embedding_size=4e-05))}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bm25_with_title_ottqa_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabfact Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 38.44it/s]00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 33.59it/s]\n",
      "Retrieving Tables for tabfact...: 100%|██████████| 12779/12779 [00:13<00:00, 964.36it/s] \n"
     ]
    }
   ],
   "source": [
    "results_bm25_with_title_tabfact_test = target_tabfact.run(bm25_with_title, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./bm25_with_title_tabfact_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'tabfact': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.33813287424681115, precision=None, recall=None, retrieval_time=9.90405, avg_retrieval_time=0.00078), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=4.63357, avg_embedding_creation_duration=0.00273, embedding_size=2.83443, avg_embedding_size=0.00167))}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bm25_with_title_tabfact_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819e1b87b36348bc89d353444171abc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 132 files:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 579.16it/s]0:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 184.00it/s]\n",
      "Retrieving Tables for spider-test...: 100%|██████████| 2147/2147 [00:29<00:00, 73.58it/s] \n"
     ]
    }
   ],
   "source": [
    "results_bm25_with_title_spider_test = target_spider.run(bm25_with_title, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./bm25_with_title_spider_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'spider-test': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.8602701443875175, precision=None, recall=None, retrieval_time=1.97192, avg_retrieval_time=0.00092), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=3.78306, avg_embedding_creation_duration=0.02102, embedding_size=0.00819, avg_embedding_size=5e-05))}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bm25_with_title_spider_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTTQA TFIDF without Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import OTTQARetriever\n",
    "tfidf_no_title = OTTQARetriever(encoding=\"tfidf\", withtitle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetaqa Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tfidf_no_title_fetaqa_test = target_fetaqa.run(tfidf_no_title, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./tfidf_no_title_fetaqa_test_retrieval_results.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "target",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
