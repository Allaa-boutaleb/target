{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import LlamaIndexRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 17:33:18.315077: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-05 17:33:18.319131: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-05 17:33:18.374084: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-05 17:33:19.595703: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from target_benchmark.evaluators import TARGET, get_task_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fact Verification Task',\n",
       " 'Table Question Answering Task',\n",
       " 'Table Retrieval Task',\n",
       " 'Text to SQL Task']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_task_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fetaqa': HFDatasetConfigDataModel(dataset_name='fetaqa', split='test', data_directory=None, query_type='Table Question Answering', aux=None, hf_corpus_dataset_path='target-benchmark/fetaqa-corpus', hf_queries_dataset_path='target-benchmark/fetaqa-queries'),\n",
       " 'ottqa': HFDatasetConfigDataModel(dataset_name='ottqa', split='test', data_directory=None, query_type='Table Question Answering', aux=None, hf_corpus_dataset_path='target-benchmark/ottqa-corpus', hf_queries_dataset_path='target-benchmark/ottqa-queries')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from target_benchmark.tasks import QuestionAnsweringTask\n",
    "QuestionAnsweringTask._get_default_dataset_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = TARGET((\"Table Question Answering Task\", \"fetaqa\"))\n",
    "llamaindex_retriever = LlamaIndexRetriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "construct_table_info() missing 1 required positional argument: 'existing_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllamaindex_retriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/carl/target/target_benchmark/evaluators/TARGET.py:478\u001b[0m, in \u001b[0;36mTARGET.run\u001b[0;34m(self, retriever, split, batch_size, top_k, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m     (\n\u001b[1;32m    472\u001b[0m         duration,\n\u001b[1;32m    473\u001b[0m         embedding_size,\n\u001b[1;32m    474\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_with_standardized_embeddings(\n\u001b[1;32m    475\u001b[0m         retriever, dataset_name, client\n\u001b[1;32m    476\u001b[0m     )\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m     duration, embedding_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_with_custom_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m loaded_datasets\u001b[38;5;241m.\u001b[39madd(dataset_name)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# create embedding statistics data object to record latency & size of embedding\u001b[39;00m\n",
      "File \u001b[0;32m~/carl/target/target_benchmark/evaluators/TARGET.py:401\u001b[0m, in \u001b[0;36mTARGET.embed_with_custom_embeddings\u001b[0;34m(self, retriever, dataset_name, batch_size)\u001b[0m\n\u001b[1;32m    398\u001b[0m start_disk_usage \u001b[38;5;241m=\u001b[39m shutil\u001b[38;5;241m.\u001b[39mdisk_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mused\n\u001b[1;32m    399\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m--> 401\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_corpus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_corpus_table_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_expected_corpus_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m    408\u001b[0m duration \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/carl/target/target_benchmark/retrievers/llama_index/LlamaIndexRetriever.py:87\u001b[0m, in \u001b[0;36mLlamaIndexRetriever.embed_corpus\u001b[0;34m(self, dataset_name, corpus)\u001b[0m\n\u001b[1;32m     85\u001b[0m db_id \u001b[38;5;241m=\u001b[39m entry_batch[DATABASE_ID_COL_NAME][i]\n\u001b[1;32m     86\u001b[0m table_id \u001b[38;5;241m=\u001b[39m entry_batch[TABLE_ID_COL_NAME][i]\n\u001b[0;32m---> 87\u001b[0m table_info \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_table_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m table_info:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma valid table name cannot be generated!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: construct_table_info() missing 1 required positional argument: 'existing_names'"
     ]
    }
   ],
   "source": [
    "results = target.run(llamaindex_retriever, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "target",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
