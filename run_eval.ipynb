{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 19:52:18.878946: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-16 19:52:18.882394: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-16 19:52:18.933449: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 19:52:20.054691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Fact Verification Task',\n",
       " 'Table Question Answering Task',\n",
       " 'Table Retrieval Task',\n",
       " 'Text to SQL Task']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from target_benchmark.evaluators import TARGET, get_task_names\n",
    "get_task_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fetaqa': HFDatasetConfigDataModel(dataset_name='fetaqa', split='test', data_directory=None, query_type='Table Question Answering', aux=None, hf_corpus_dataset_path='target-benchmark/fetaqa-corpus', hf_queries_dataset_path='target-benchmark/fetaqa-queries'),\n",
       " 'ottqa': HFDatasetConfigDataModel(dataset_name='ottqa', split='validation', data_directory=None, query_type='Table Question Answering', aux=None, hf_corpus_dataset_path='target-benchmark/ottqa-corpus', hf_queries_dataset_path='target-benchmark/ottqa-queries'),\n",
       " 'tabfact': HFDatasetConfigDataModel(dataset_name='tabfact', split='test', data_directory=None, query_type='Fact Verification', aux=None, hf_corpus_dataset_path='target-benchmark/tabfact-corpus', hf_queries_dataset_path='target-benchmark/tabfact-queries'),\n",
       " 'infiagentda': HFDatasetConfigDataModel(dataset_name='infiagentda', split='test', data_directory=None, query_type=None, aux=None, hf_corpus_dataset_path='target-benchmark/infiagentda-corpus', hf_queries_dataset_path='target-benchmark/infiagentda-queries'),\n",
       " 'spider-test': HFDatasetConfigDataModel(dataset_name='spider-test', split='test', data_directory=None, query_type='Text to SQL', aux=None, hf_corpus_dataset_path='target-benchmark/spider-corpus-test', hf_queries_dataset_path='target-benchmark/spider-queries-test')}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from target_benchmark.tasks import TableRetrievalTask\n",
    "TableRetrievalTask._get_default_dataset_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fetaqa = TARGET((\"Table Retrieval Task\", \"fetaqa\"))\n",
    "target_ottqa = TARGET((\"Table Retrieval Task\", \"ottqa\"))\n",
    "target_tabfact = TARGET((\"Table Retrieval Task\", \"tabfact\"))\n",
    "target_spider = TARGET((\"Table Retrieval Task\", \"spider-test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import LlamaIndexRetriever\n",
    "llamaindex_retriever = LlamaIndexRetriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = target_fetaqa.run(llamaindex_retriever, \"validation\", top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=5, accuracy=0.44555444555444557, precision=None, recall=None, retrieval_time=106.81327, avg_retrieval_time=0.10671), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'bleu': {'bleu': 0.12774350688956024, 'precisions': [0.35200152134639157, 0.17261518494484102, 0.10533781653882526, 0.0676979374584165], 'brevity_penalty': 0.8854071251608363, 'length_ratio': 0.8914978384335001, 'translation_length': 21034, 'reference_length': 23594}, 'sacrebleu': {'score': 12.774350688956028, 'counts': [7404, 3458, 2005, 1221], 'totals': [21034, 20033, 19034, 18036], 'precisions': [35.200152134639154, 17.2615184944841, 10.533781653882526, 6.76979374584165], 'bp': 0.8854071251608363, 'sys_len': 21034, 'ref_len': 23594}, 'rouge': {'rouge1': 0.2797871142743029, 'rouge2': 0.16322956616548168, 'rougeL': 0.22260816489324553, 'rougeLsum': 0.22779389901592179}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=43.32646, avg_embedding_creation_duration=0.04328, embedding_size=3.52666, avg_embedding_size=0.00352))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.5454545454545454, precision=None, recall=None, retrieval_time=105.64053, avg_retrieval_time=0.10553), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'rouge': {'rouge1': 0.3416405652971404, 'rouge2': 0.20253177988132284, 'rougeL': 0.2754744438590071, 'rougeLsum': 0.2818891819491625}, 'sacrebleu': {'score': 14.875547927665979, 'counts': [8880, 4277, 2498, 1525], 'totals': [24841, 23840, 22841, 21844], 'precisions': [35.74735316613663, 17.940436241610737, 10.936473884681057, 6.981322102179088], 'bp': 1.0, 'sys_len': 24841, 'ref_len': 23594}, 'bleu': {'bleu': 0.1487554792766598, 'precisions': [0.3574735316613663, 0.17940436241610738, 0.10936473884681056, 0.06981322102179088], 'brevity_penalty': 1.0, 'length_ratio': 1.0528524201068068, 'translation_length': 24841, 'reference_length': 23594}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=42.41042, avg_embedding_creation_duration=0.04237, embedding_size=-0.00819, avg_embedding_size=-1e-05))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetaqa Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = target_fetaqa.run(llamaindex_retriever, \"test\", top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.44782825761357964, precision=None, recall=None, retrieval_time=414.04305, avg_retrieval_time=0.20671), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'sacrebleu': {'score': 12.834750303393088, 'counts': [16461, 7853, 4664, 2900], 'totals': [53432, 51429, 49431, 47434], 'precisions': [30.807381344512653, 15.269594975597425, 9.435374562521494, 6.113758063836067], 'bp': 1.0, 'sys_len': 53432, 'ref_len': 46919}, 'rouge': {'rouge1': 0.30684692336094793, 'rouge2': 0.18092410771505416, 'rougeL': 0.2514185751479096, 'rougeLsum': 0.2551126236872021}, 'bleu': {'bleu': 0.1283475030339309, 'precisions': [0.3080738134451265, 0.15269594975597425, 0.09435374562521495, 0.06113758063836067], 'brevity_penalty': 1.0, 'length_ratio': 1.1388137002067393, 'translation_length': 53432, 'reference_length': 46919}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=163.07156, avg_embedding_creation_duration=0.08141, embedding_size=-6.47987, avg_embedding_size=-0.00324))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTTQA Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_llama_ottqa_val = target_ottqa.run(llamaindex_retriever, \"validation\", top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabfact Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:target_benchmark.evaluators.TARGET:Started creating data loader objects...\n",
      "INFO:target_benchmark.evaluators.TARGET:Start running Fact Verification Task...\n",
      "INFO:target_benchmark.evaluators.TARGET:Start checking for new corpus to embed...\n"
     ]
    }
   ],
   "source": [
    "results_llama_tabfact_test = target_tabfact.run(llamaindex_retriever, \"test\", top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_llama_tabfact_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Naive OAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import OpenAIEmbedder\n",
    "oai_embedder = OpenAIEmbedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetaqa Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Tables...: 100%|██████████| 2003/2003 [04:32<00:00,  7.36it/s]\n"
     ]
    }
   ],
   "source": [
    "results_oai_fetaqa_test = target_fetaqa.run(oai_embedder, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./oai_fetaqa_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.7240584166026134, precision=None, recall=None, retrieval_time=993.16343, avg_retrieval_time=0.49584), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=15.94904, avg_embedding_creation_duration=0.00796, embedding_size=24612864.0, avg_embedding_size=12288.0))}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_oai_fetaqa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTTQA Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Tables...: 100%|██████████| 789/789 [01:56<00:00,  6.75it/s]\n",
      "Retrieving Tables for ottqa...: 100%|██████████| 2214/2214 [05:41<00:00,  6.49it/s]\n"
     ]
    }
   ],
   "source": [
    "results_oai_ottqa_val = target_ottqa.run(oai_embedder, \"validation\", top_k = 10, batch_size=100, retrieval_results_file=\"./oai_ottqa_val_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'ottqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.9498644986449865, precision=None, recall=None, retrieval_time=1101.52391, avg_retrieval_time=0.49753), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=6.22053, avg_embedding_creation_duration=0.00788, embedding_size=9695232.0, avg_embedding_size=12288.0))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_oai_ottqa_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabfact Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Tables...: 100%|██████████| 1695/1695 [04:00<00:00,  7.06it/s]\n",
      "Retrieving Tables for tabfact...: 100%|██████████| 12779/12779 [32:16<00:00,  6.60it/s] \n"
     ]
    }
   ],
   "source": [
    "results_oai_tabfact_test = target_tabfact.run(oai_embedder, \"test\", top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'tabfact': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.7829251115110728, precision=None, recall=None, retrieval_time=6411.06452, avg_retrieval_time=0.50169), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=14.1249, avg_embedding_creation_duration=0.00833, embedding_size=20828160.0, avg_embedding_size=12288.0))}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_oai_tabfact_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653996385ca54f149da7876c4ef04887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Tables...:   0%|          | 0/755 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys of the corpus: dict_keys(['text'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'table'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_oai_spider_test \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_spider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moai_embedder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieval_results_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./oai_spider_test_retrieval_results.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/carl/target/target_benchmark/evaluators/TARGET.py:482\u001b[0m, in \u001b[0;36mTARGET.run\u001b[0;34m(self, retriever, split, batch_size, top_k, retrieval_results_file, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m duration, embedding_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m standardized:\n\u001b[1;32m    479\u001b[0m     (\n\u001b[1;32m    480\u001b[0m         duration,\n\u001b[1;32m    481\u001b[0m         embedding_size,\n\u001b[0;32m--> 482\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_with_standardized_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     duration, embedding_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_with_custom_embeddings(\n\u001b[1;32m    487\u001b[0m         retriever, dataset_name, batch_size\n\u001b[1;32m    488\u001b[0m     )\n",
      "File \u001b[0;32m~/carl/target/target_benchmark/evaluators/TARGET.py:371\u001b[0m, in \u001b[0;36mTARGET.embed_with_standardized_embeddings\u001b[0;34m(self, retriever, dataset_name, client)\u001b[0m\n\u001b[1;32m    369\u001b[0m metadata \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    370\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m--> 371\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcur_dataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_corpus_table_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_expected_corpus_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEmbedding Tables...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO: support batching\u001b[39;49;00m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable_embedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/target/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/carl/target/target_benchmark/dataset_loaders/AbsDatasetLoader.py:153\u001b[0m, in \u001b[0;36mAbsDatasetLoader.convert_corpus_table_to\u001b[0;34m(self, output_format, batch_size)\u001b[0m\n\u001b[1;32m    149\u001b[0m     dict_tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28mmap\u001b[39m(array_of_arrays_to_dict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus[TABLE_COL_NAME])\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    152\u001b[0m     converted_corpus[TABLE_COL_NAME] \u001b[38;5;241m=\u001b[39m dict_tables\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mconverted_corpus\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTABLE_COL_NAME\u001b[49m\u001b[43m]\u001b[49m), batch_size):\n\u001b[1;32m    154\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Use list comprehensions to extract each column\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'table'"
     ]
    }
   ],
   "source": [
    "results_oai_spider_test = target_spider.run(oai_embedder, \"test\", top_k=10, batch_size=100, retrieval_results_file=\"./oai_spider_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HNSW OAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import HNSWOpenAIEmbeddingRetriever\n",
    "hnsw_oai = HNSWOpenAIEmbeddingRetriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_hnsw_oai_fetaqa_test = target_fetaqa.run(hnsw_oai, \"test\", top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.12431352970544184, precision=None, recall=None, retrieval_time=24.25388, avg_retrieval_time=0.01211), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'rouge': {'rouge1': 0.0943664578019575, 'rouge2': 0.049193712730089476, 'rougeL': 0.07689053600356258, 'rougeLsum': 0.07803228871432272}, 'sacrebleu': {'score': 3.4375341088803752, 'counts': [6235, 1989, 1080, 620], 'totals': [27543, 25540, 23539, 21538], 'precisions': [22.637330719239007, 7.78778386844166, 4.588130336887718, 2.8786331135667194], 'bp': 0.49485928556419767, 'sys_len': 27543, 'ref_len': 46919}, 'bleu': {'bleu': 0.034375341088803746, 'precisions': [0.22637330719239007, 0.0778778386844166, 0.045881303368877184, 0.028786331135667195], 'brevity_penalty': 0.49485928556419767, 'length_ratio': 0.5870329717172148, 'translation_length': 27543, 'reference_length': 46919}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=8.7827, avg_embedding_creation_duration=0.00438, embedding_size=12.72218, avg_embedding_size=0.00635))}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_hnsw_oai_fetaqa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTTQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import OTTQARetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ottqa_tfidf = OTTQARetriever(encoding=\"bm25\")\n",
    "results_ottqa_tfidf_fetaqa_test = target_fetaqa.run(ottqa_tfidf, \"test\", top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "target",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
