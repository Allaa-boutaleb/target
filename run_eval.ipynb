{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 06:06:22.430349: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-16 06:06:22.439119: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-16 06:06:22.491587: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 06:06:23.584895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from target_benchmark.evaluators import TARGET, get_task_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fact Verification Task',\n",
       " 'Table Question Answering Task',\n",
       " 'Table Retrieval Task',\n",
       " 'Text to SQL Task']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_task_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fetaqa': HFDatasetConfigDataModel(dataset_name='fetaqa', split='test', data_directory=None, query_type='Table Question Answering', aux=None, hf_corpus_dataset_path='target-benchmark/fetaqa-corpus', hf_queries_dataset_path='target-benchmark/fetaqa-queries'),\n",
       " 'ottqa': HFDatasetConfigDataModel(dataset_name='ottqa', split='validation', data_directory=None, query_type='Table Question Answering', aux=None, hf_corpus_dataset_path='target-benchmark/ottqa-corpus', hf_queries_dataset_path='target-benchmark/ottqa-queries'),\n",
       " 'tabfact': HFDatasetConfigDataModel(dataset_name='tabfact', split='test', data_directory=None, query_type='Fact Verification', aux=None, hf_corpus_dataset_path='target-benchmark/tabfact-corpus', hf_queries_dataset_path='target-benchmark/tabfact-queries'),\n",
       " 'infiagentda': HFDatasetConfigDataModel(dataset_name='infiagentda', split='test', data_directory=None, query_type=None, aux=None, hf_corpus_dataset_path='target-benchmark/infiagentda-corpus', hf_queries_dataset_path='target-benchmark/infiagentda-queries'),\n",
       " 'spider-test': HFDatasetConfigDataModel(dataset_name='spider-test', split='test', data_directory=None, query_type='Text to SQL', aux=None, hf_corpus_dataset_path='target-benchmark/spider-corpus-test', hf_queries_dataset_path='target-benchmark/spider-queries-test')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from target_benchmark.tasks import TableRetrievalTask\n",
    "TableRetrievalTask._get_default_dataset_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fetaqa = TARGET((\"Table Retrieval Task\", \"fetaqa\"))\n",
    "target_ottqa = TARGET((\"Table Retrieval Task\", \"ottqa\"))\n",
    "target_tabfact = TARGET((\"Table Retrieval Task\", \"tabfact\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import LlamaIndexRetriever\n",
    "llamaindex_retriever = LlamaIndexRetriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = target_fetaqa.run(llamaindex_retriever, \"validation\", top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=5, accuracy=0.44555444555444557, precision=None, recall=None, retrieval_time=106.81327, avg_retrieval_time=0.10671), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'bleu': {'bleu': 0.12774350688956024, 'precisions': [0.35200152134639157, 0.17261518494484102, 0.10533781653882526, 0.0676979374584165], 'brevity_penalty': 0.8854071251608363, 'length_ratio': 0.8914978384335001, 'translation_length': 21034, 'reference_length': 23594}, 'sacrebleu': {'score': 12.774350688956028, 'counts': [7404, 3458, 2005, 1221], 'totals': [21034, 20033, 19034, 18036], 'precisions': [35.200152134639154, 17.2615184944841, 10.533781653882526, 6.76979374584165], 'bp': 0.8854071251608363, 'sys_len': 21034, 'ref_len': 23594}, 'rouge': {'rouge1': 0.2797871142743029, 'rouge2': 0.16322956616548168, 'rougeL': 0.22260816489324553, 'rougeLsum': 0.22779389901592179}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=43.32646, avg_embedding_creation_duration=0.04328, embedding_size=3.52666, avg_embedding_size=0.00352))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.5454545454545454, precision=None, recall=None, retrieval_time=105.64053, avg_retrieval_time=0.10553), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'rouge': {'rouge1': 0.3416405652971404, 'rouge2': 0.20253177988132284, 'rougeL': 0.2754744438590071, 'rougeLsum': 0.2818891819491625}, 'sacrebleu': {'score': 14.875547927665979, 'counts': [8880, 4277, 2498, 1525], 'totals': [24841, 23840, 22841, 21844], 'precisions': [35.74735316613663, 17.940436241610737, 10.936473884681057, 6.981322102179088], 'bp': 1.0, 'sys_len': 24841, 'ref_len': 23594}, 'bleu': {'bleu': 0.1487554792766598, 'precisions': [0.3574735316613663, 0.17940436241610738, 0.10936473884681056, 0.06981322102179088], 'brevity_penalty': 1.0, 'length_ratio': 1.0528524201068068, 'translation_length': 24841, 'reference_length': 23594}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=42.41042, avg_embedding_creation_duration=0.04237, embedding_size=-0.00819, avg_embedding_size=-1e-05))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetaqa Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = target_fetaqa.run(llamaindex_retriever, \"test\", top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.44782825761357964, precision=None, recall=None, retrieval_time=414.04305, avg_retrieval_time=0.20671), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'sacrebleu': {'score': 12.834750303393088, 'counts': [16461, 7853, 4664, 2900], 'totals': [53432, 51429, 49431, 47434], 'precisions': [30.807381344512653, 15.269594975597425, 9.435374562521494, 6.113758063836067], 'bp': 1.0, 'sys_len': 53432, 'ref_len': 46919}, 'rouge': {'rouge1': 0.30684692336094793, 'rouge2': 0.18092410771505416, 'rougeL': 0.2514185751479096, 'rougeLsum': 0.2551126236872021}, 'bleu': {'bleu': 0.1283475030339309, 'precisions': [0.3080738134451265, 0.15269594975597425, 0.09435374562521495, 0.06113758063836067], 'brevity_penalty': 1.0, 'length_ratio': 1.1388137002067393, 'translation_length': 53432, 'reference_length': 46919}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=163.07156, avg_embedding_creation_duration=0.08141, embedding_size=-6.47987, avg_embedding_size=-0.00324))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTTQA Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_llama_ottqa_val = target_ottqa.run(llamaindex_retriever, \"validation\", top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabfact Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:target_benchmark.evaluators.TARGET:Started creating data loader objects...\n",
      "INFO:target_benchmark.evaluators.TARGET:Start running Fact Verification Task...\n",
      "INFO:target_benchmark.evaluators.TARGET:Start checking for new corpus to embed...\n"
     ]
    }
   ],
   "source": [
    "results_llama_tabfact_test = target_tabfact.run(llamaindex_retriever, \"test\", top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_llama_tabfact_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Naive OAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import OpenAIEmbedder\n",
    "oai_embedder = OpenAIEmbedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetaqa Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Tables...: 100%|██████████| 2003/2003 [04:32<00:00,  7.36it/s]\n"
     ]
    }
   ],
   "source": [
    "results_oai_fetaqa_test = target_fetaqa.run(oai_embedder, \"test\", top_k = 10, batch_size=100, retrieval_results_file=\"./oai_fetaqa_test_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Retrieval Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.7240584166026134, precision=None, recall=None, retrieval_time=993.16343, avg_retrieval_time=0.49584), downstream_task_performance=DownstreamTaskPerformanceDataModel(task_name=None, scores=None), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=15.94904, avg_embedding_creation_duration=0.00796, embedding_size=24612864.0, avg_embedding_size=12288.0))}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_oai_fetaqa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTTQA Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_oai_ottqa_val = target_ottqa.run(oai_embedder, \"validation\", top_k = 10, batch_size=100, retrieval_results_file=\"./oai_ottqa_val_retrieval_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'ottqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.9498644986449865, precision=None, recall=None, retrieval_time=25.55162, avg_retrieval_time=0.01154), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'rouge': {'rouge1': 0.052696471756130944, 'rouge2': 0.023800247076971245, 'rougeL': 0.051943622206043066, 'rougeLsum': 0.05206516546355007}, 'sacrebleu': {'score': 0.7781940221698809, 'counts': [961, 365, 136, 62], 'totals': [33360, 31146, 28959, 26803], 'precisions': [2.880695443645084, 1.1719000834778142, 0.46962947615594464, 0.2313173898444204], 'bp': 1.0, 'sys_len': 33360, 'ref_len': 4925}, 'bleu': {'bleu': 0.007781940221698806, 'precisions': [0.02880695443645084, 0.011719000834778141, 0.004696294761559446, 0.002313173898444204], 'brevity_penalty': 1.0, 'length_ratio': 6.773604060913706, 'translation_length': 33360, 'reference_length': 4925}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=5.91824, avg_embedding_creation_duration=0.0075, embedding_size=9695232.0, avg_embedding_size=12288.0))}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_oai_ottqa_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabfact Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_oai_tabfact_test = target_tabfact.run(oai_embedder, \"test\", top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_oai_tabfact_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HNSW OAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import HNSWOpenAIEmbeddingRetriever\n",
    "hnsw_oai = HNSWOpenAIEmbeddingRetriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_hnsw_oai_fetaqa_test = target_fetaqa.run(hnsw_oai, \"test\", top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=10, accuracy=0.12431352970544184, precision=None, recall=None, retrieval_time=24.25388, avg_retrieval_time=0.01211), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'rouge': {'rouge1': 0.0943664578019575, 'rouge2': 0.049193712730089476, 'rougeL': 0.07689053600356258, 'rougeLsum': 0.07803228871432272}, 'sacrebleu': {'score': 3.4375341088803752, 'counts': [6235, 1989, 1080, 620], 'totals': [27543, 25540, 23539, 21538], 'precisions': [22.637330719239007, 7.78778386844166, 4.588130336887718, 2.8786331135667194], 'bp': 0.49485928556419767, 'sys_len': 27543, 'ref_len': 46919}, 'bleu': {'bleu': 0.034375341088803746, 'precisions': [0.22637330719239007, 0.0778778386844166, 0.045881303368877184, 0.028786331135667195], 'brevity_penalty': 0.49485928556419767, 'length_ratio': 0.5870329717172148, 'translation_length': 27543, 'reference_length': 46919}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=8.7827, avg_embedding_creation_duration=0.00438, embedding_size=12.72218, avg_embedding_size=0.00635))}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_hnsw_oai_fetaqa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTTQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import OTTQARetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ottqa_tfidf = OTTQARetriever(encoding=\"bm25\")\n",
    "results_ottqa_tfidf_fetaqa_test = target_fetaqa.run(ottqa_tfidf, \"test\", top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "target",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
