{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_benchmark.retrievers import LlamaIndexRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 05:21:41.216198: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-06 05:21:41.219910: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-06 05:21:41.274303: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-06 05:21:42.387597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from target_benchmark.evaluators import TARGET, get_task_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fact Verification Task',\n",
       " 'Table Question Answering Task',\n",
       " 'Table Retrieval Task',\n",
       " 'Text to SQL Task']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_task_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fetaqa': HFDatasetConfigDataModel(dataset_name='fetaqa', split='test', data_directory=None, query_type='Table Question Answering', aux=None, hf_corpus_dataset_path='target-benchmark/fetaqa-corpus', hf_queries_dataset_path='target-benchmark/fetaqa-queries'),\n",
       " 'ottqa': HFDatasetConfigDataModel(dataset_name='ottqa', split='test', data_directory=None, query_type='Table Question Answering', aux=None, hf_corpus_dataset_path='target-benchmark/ottqa-corpus', hf_queries_dataset_path='target-benchmark/ottqa-queries')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from target_benchmark.tasks import QuestionAnsweringTask\n",
    "QuestionAnsweringTask._get_default_dataset_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = TARGET((\"Table Question Answering Task\", \"fetaqa\"))\n",
    "llamaindex_retriever = LlamaIndexRetriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = target.run(llamaindex_retriever, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table Question Answering Task': {'fetaqa': TaskResultsDataModel(retrieval_performance=RetrievalPerformanceDataModel(k=5, accuracy=0.44555444555444557, precision=None, recall=None, retrieval_time=106.81327, avg_retrieval_time=0.10671), downstream_task_performance=TableQATaskPerformanceDataModel(task_name='Table Question Answering Task', scores={'bleu': {'bleu': 0.12774350688956024, 'precisions': [0.35200152134639157, 0.17261518494484102, 0.10533781653882526, 0.0676979374584165], 'brevity_penalty': 0.8854071251608363, 'length_ratio': 0.8914978384335001, 'translation_length': 21034, 'reference_length': 23594}, 'sacrebleu': {'score': 12.774350688956028, 'counts': [7404, 3458, 2005, 1221], 'totals': [21034, 20033, 19034, 18036], 'precisions': [35.200152134639154, 17.2615184944841, 10.533781653882526, 6.76979374584165], 'bp': 0.8854071251608363, 'sys_len': 21034, 'ref_len': 23594}, 'rouge': {'rouge1': 0.2797871142743029, 'rouge2': 0.16322956616548168, 'rougeL': 0.22260816489324553, 'rougeLsum': 0.22779389901592179}}), embedding_statistics=EmbeddingStatisticsDataModel(embedding_creation_duration=43.32646, avg_embedding_creation_duration=0.04328, embedding_size=3.52666, avg_embedding_size=0.00352))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "target",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
